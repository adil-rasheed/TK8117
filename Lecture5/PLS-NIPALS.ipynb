{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIPALS: Non-linear Iterative PArtial Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[4,9,6,7,7,8,3,2],\n",
    "              [6,15,10,15,17,22,9,4],\n",
    "              [8,21,14,23,27,36,15,6],\n",
    "              [10,21,14,13,11,10,3,4],\n",
    "              [12,27,18,21,21,24,9,6],\n",
    "              [14,33,22,29,31,38,15,8],\n",
    "              [16,33,22,19,15,12,3,6],\n",
    "              [18,39,26,27,25,26,9,8],\n",
    "              [20,45,30,35,35,40,15,10]])\n",
    "y = np.array([[1, 1],[3, 1],[5, 1],[1, 3],[3, 3],[5, 3],[1 ,5],[3, 5],[5 ,5]])\n",
    "X=x[:-1,:]\n",
    "Y=y[:-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nipals.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X}_{a}=normalize(\\mathbf{X})$$\n",
    "$$\\mathbf{Y}_{a}=normalize(\\mathbf{Y})$$\n",
    "\n",
    "* Arrow 1: Perform ùêæ regressions, regressing each column from $\\mathbf{X}_{a}$ onto the vector $\\mathbf{u}_{a}$. The slope coefficients from the regressions are stored as the entries in $\\mathbf{w}_{a}$. Columns in $\\mathbf{X}_{a}$ which are strongly correlated with $\\mathbf{u}_{a}$ will have large weights in $\\mathbf{w}_{a}$, while unrelated columns will have small, close to zero, weights. We can perform these regression in one go:\n",
    "$$\\mathbf{w}_{a}=(\\mathbf{u}_{a}^{'}\\mathbf{u}_{a})^{-1}\\mathbf{X}_{a}^{'}\\mathbf{u}_{a}$$\n",
    "* Normalize the weight vector to unit length:\n",
    "$$\\mathbf{w}_{a}=\\frac{\\mathbf{w}_{a}}{\\sqrt{\\mathbf{w}_{a}^{'}\\mathbf{w}_{a}}}$$\n",
    "* Regress every row in $\\mathbf{x}_{a}$ onto the weight vector. The slope coefficients are stored as entries in $\\mathbf{t}_{a}$. This means that rows in $\\mathbf{x}_{a}$ that have a similar pattern to that described by the weight vector will have large values in $\\mathbf{t}_{a}$. Observations that are totally different to $\\mathbf{w}_{a}$ will have near-zero score values. These $N$ regressions can be performed in one go:\n",
    "$$\\mathbf{t}_{a}=(\\mathbf{w}_{a}^{'}\\mathbf{w}_{a})^{-1}\\mathbf{X}_{a}^{'}\\mathbf{w}_{a}$$\n",
    "* Arrow 4 Next, regress every column in $\\mathbf{y}_{a}$ onto this score vector, $\\mathbf{t}_{a}$. The slope coefficients are stored in $\\mathbf{q}_{a}$. We can calculate all $M$ slope coefficients:\n",
    "$$\\mathbf{q}_{a}=(\\mathbf{t}_{a}^{'}\\mathbf{t}_{a})^{-1}\\mathbf{Y}_{a}^{'}\\mathbf{t}_{a}$$\n",
    "Arrow 5 Finally, regress each of the $N$ rows in $\\mathbf{y}_{a}$ onto this weight vector, $\\mathbf{q}_{a}$. Observations in $\\mathbf{y}_{a}$ that are strongly related to $\\mathbf{q}_{a}$ will have large positive or negative slope coefficients in vector $\\mathbf{u}_{a}$:\n",
    "$$\\mathbf{u}_{a}=(\\mathbf{q}_{a}^{'}\\mathbf{q}_{a})^{-1}\\mathbf{Y}_{a}\\mathbf{q}_{a}$$\n",
    "This is one round of the NIPALS algorithm. We iterate through these 4 arrow steps until the $\\mathbf{u}_{a}$ vector does not change much. On convergence, we store these 4 vectors: $\\mathbf{w}_{a}$, $\\mathbf{t}_{a}$, $\\mathbf{q}_{a}$ and $\\mathbf{u}_{a}$ which jointly define the $a^{th}$ component.\n",
    "Then we deflate. Deflation removes variability already explained from $\\mathbf{X}_{a}$ and $\\mathbf{Y}_{a}$. Deflation proceeds as follows:\n",
    "\n",
    "Step 1: Calculate a loadings vector for the $X$ space, $\\mathbf{p}_{a}$ called using the X-space scores: \n",
    "$$\\mathbf{p}_{a}=(\\mathbf{t}_{a}^{'}\\mathbf{t}_{a})^{-1}\\mathbf{X}_{a}^{'}\\mathbf{t}_{a}$$\n",
    "Step 2: Remove the predicted variability from $X$ and $Y$ Using the loadings, $\\mathbf{p}_{a}$ just calculated\n",
    "above, we remove from $\\mathbf{X}_{a}$ the best prediction of $\\mathbf{p}_{a}$, in other words, remove everything we can explain about it. We also remove any variance explained from $\\mathbf{Y}_{a}$:\n",
    "$$\\mathbf{\\hat{X}}_{a}=\\mathbf{t}_{a}\\mathbf{p}_{a}^{'}$$\n",
    "$$\\mathbf{\\hat{Y}}_{a}=\\mathbf{t}_{a}\\mathbf{q}_{a}^{'}$$\n",
    "$$\\mathbf{X}_{a}=\\mathbf{X}_{a}-\\mathbf{\\hat{X}}_{a}$$\n",
    "$$\\mathbf{Y}_{a}=\\mathbf{Y}_{a}-\\mathbf{\\hat{Y}}_{a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls(X, Y, no_components):\n",
    "    MAXITER=10000\n",
    "    CC=0.000000001\n",
    "    Xc = (X-X.mean(axis=0))/X.std(axis=0,ddof=1)\n",
    "    Yc = (Y-Y.mean(axis=0))/Y.std(axis=0,ddof=1)\n",
    "    data_samples,X_variables = X.shape\n",
    "    data_samples,Y_variables = Y.shape\n",
    "    \n",
    "    W = np.empty((X_variables, no_components))\n",
    "    T = np.empty((data_samples, no_components))\n",
    "    Q = np.empty((Y_variables, no_components))\n",
    "    U = np.empty((data_samples, no_components))\n",
    "    P = np.empty((X_variables, no_components))\n",
    "    c = np.empty((no_components,))\n",
    "    components = 0\n",
    "    Xa = Xc\n",
    "    Ya = Yc\n",
    "\n",
    "    for j in range(0, no_components):\n",
    "        ua = Ya[:,0]\n",
    "        ITER = 0\n",
    "        delU = CC * 10.0\n",
    "\n",
    "        while ITER < MAXITER and delU > CC:                      \n",
    "            wa = np.dot(Xa.T,ua)/np.dot(ua.T,ua)\n",
    "            wa = wa/np.linalg.norm(wa, 2)\n",
    "            ta = np.dot(Xa,wa)/np.dot(wa.T,wa)\n",
    "            qa = np.dot(Ya.T,ta)/np.dot(ta.T,ta)\n",
    "            qa = qa/np.linalg.norm(qa, 2)\n",
    "            old_ua = ua\n",
    "            ua = np.dot(Ya,qa)/np.dot(qa.T,qa)\n",
    "            delU = np.linalg.norm(ua - old_ua)\n",
    "            ITER += 1\n",
    "\n",
    "        if ITER >= MAXITER:\n",
    "            if ignore_failures:\n",
    "                break\n",
    "            else:\n",
    "                raise ConvergenceError('PLS failed to converge for ' 'component: ' '{}'.format(components+1))\n",
    "        W[:, j] = wa\n",
    "        T[:, j] = ta\n",
    "        Q[:, j] = qa\n",
    "        U[:, j] = ua\n",
    "\n",
    "        P[:, j] = np.dot(Xa.T,ta) / np.dot(ta.T,ta)\n",
    "        Xa = Xa - np.outer(ta, P[:, j].T)\n",
    "        Ya = Ya -  np.outer(ta, qa.T)\n",
    "    return T,W,U,Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "[T,W,U,Q,P] = pls(X,Y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81589566,  0.62819786],\n",
       "       [ 0.57819917, -0.77805363]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46171289,  0.41623171],\n",
       "       [-0.48403379,  1.00515512],\n",
       "       [ 0.49364531,  1.59407852],\n",
       "       [-0.76886297, -0.64838508],\n",
       "       [ 0.20881613, -0.05946167],\n",
       "       [ 1.18649523,  0.52946173],\n",
       "       [-0.07601305, -1.71300187],\n",
       "       [ 0.90166605, -1.12407846]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.13421324,  0.3113111 ],\n",
       "       [-1.5200581 ,  1.17675963],\n",
       "       [ 1.09409705,  2.04220815],\n",
       "       [-2.02355325, -0.90992154],\n",
       "       [ 0.59060189, -0.04447301],\n",
       "       [ 3.20475704,  0.82097551],\n",
       "       [ 0.08710674, -2.13115418],\n",
       "       [ 2.70126188, -1.26570566]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29004531, -0.4662135 ],\n",
       "       [ 0.33287536, -0.36863102],\n",
       "       [ 0.33287536, -0.36863102],\n",
       "       [ 0.40629812,  0.03248028],\n",
       "       [ 0.39020433,  0.2574492 ],\n",
       "       [ 0.34728441,  0.42452259],\n",
       "       [ 0.31368445,  0.50390541],\n",
       "       [ 0.39739116, -0.09691315]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46171289,  0.41623171],\n",
       "       [-0.48403379,  1.00515512],\n",
       "       [ 0.49364531,  1.59407852],\n",
       "       [-0.76886297, -0.64838508],\n",
       "       [ 0.20881613, -0.05946167],\n",
       "       [ 1.18649523,  0.52946173],\n",
       "       [-0.07601305, -1.71300187],\n",
       "       [ 0.90166605, -1.12407846]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
